{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IA2.0_v0001.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6/CDtFos7uTCqigPBPXc1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelounb/BB/blob/master/IA2_0_v0001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unZZy_q_AA4B"
      },
      "source": [
        "# Parâmetros para testes do modelo de IA no modelo \"modelo_treinamento\":\r\n",
        "global_vocab_size = 4000          # Tamanho máximo do vocabulário\r\n",
        "global_embedding_dim = 128        # Tamanho do batch\r\n",
        "global_max_length = 15           # Quantidade máxima de palavras em uma review \r\n",
        "num_epochs = 1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w6tUulXtOls",
        "outputId": "f53c3492-a936-4e63-dbba-7d0e82bd2c2a"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4eNogxBsftN"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxDIEtXXtE2j"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Embedding\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import Bidirectional\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers.convolutional import MaxPooling1D\r\n",
        "\r\n",
        "# For Tokenizing\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "# For N-grams\r\n",
        "from keras.preprocessing import text\r\n",
        "# For taking off the punctuation from text\r\n",
        "from unidecode import unidecode"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EDRdP8JvUKW"
      },
      "source": [
        "def reduzir_texto(frases):\r\n",
        "  frase = ''\r\n",
        "  for item in frases.lower().split():\r\n",
        "    if item not in ['a', 'o', 'as', 'os', 'da', 'do', 'das', 'dos', 'e', 'no', 'na', 'em', 'i']:  # retira os artigos e conjuncao 'e'\r\n",
        "      if item[-1] in ['.', ',', '!', ';' ]:\r\n",
        "        frase +=str(unidecode(item[0:-1]))+' '  # retira pontuacao\r\n",
        "      else:\r\n",
        "        frase +=str(unidecode(item))+' '\r\n",
        "  return frase[:-1]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def reduzir_review(review):\r\n",
        "  review_reduzida = []\r\n",
        "  for item in review:\r\n",
        "    review_reduzida.append(reduzir_texto(item))\r\n",
        "  return review_reduzida   \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def acrescentando_dimensionalidade(listagem1):\r\n",
        "  # Pega uma lista do tipo:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1] e transforma em [[1], [1], [1], [1], [1], [1], [1], [1], [1], [1]]\r\n",
        "  return [[int(item)] for item in listagem1] \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def tokenizing_review(reviews):\r\n",
        "  # Tokenizing values\r\n",
        "  vocab_size = global_vocab_size\r\n",
        "  embedding_dim = global_embedding_dim\r\n",
        "  max_length = global_max_length\r\n",
        "  oov_tok = \"<OOV>\"\r\n",
        "\r\n",
        "  tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\r\n",
        "  tokenizer.fit_on_texts(reviews)\r\n",
        "  sequences = tokenizer.texts_to_sequences(reviews)\r\n",
        "  word_index = tokenizer.word_index\r\n",
        "\r\n",
        "  # Padding the sequences\r\n",
        "  trunc_type='post'\r\n",
        "  padding_type='post'\r\n",
        "  sequences = pad_sequences(sequences,maxlen=global_max_length, padding=padding_type, truncating=trunc_type)  # Acrescentando zeros ao review\r\n",
        "  return tokenizer, sequences, word_index\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def montar_dataset(column1, column2, column3):  # Transforma 3 colunas do dataset em um unico input (score+review+colunaX) \r\n",
        "  column1 = acrescentando_dimensionalidade(column1)\r\n",
        "  column3 = acrescentando_dimensionalidade(column3)\r\n",
        "  listanova_X, listanova_y = [],[]\r\n",
        "  for i in range(len(column1)):\r\n",
        "    listanova_X.append(column1[i]+list(column2[i]))\r\n",
        "    listanova_y.append(column3[i])\r\n",
        "  lista_pandas_X = pd.DataFrame(listanova_X)\r\n",
        "  lista_pandas_y = pd.DataFrame(listanova_y)\r\n",
        "  return lista_pandas_X, lista_pandas_y\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def split_the_dataset(dataset):\r\n",
        "  len(dataset)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def print_baseline_error(model, model_id, X_test, y_test):\r\n",
        "  scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
        "  print(\"Baseline Error \" + model_id + \" : %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXN1O4WvB-nh"
      },
      "source": [
        "Definindo os modelos de IA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h70T93a5Kko"
      },
      "source": [
        "# define the model\r\n",
        "def model1(vocab_size, max_length):\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Embedding(vocab_size, 100, input_length=max_length))\r\n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\r\n",
        "  model.add(MaxPooling1D(pool_size=2))\r\n",
        "  model.add(Flatten())\r\n",
        "  model.add(Dense(10, activation='relu'))\r\n",
        "  model.add(Dense(1, activation='sigmoid'))\r\n",
        "  # compile network\r\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "  # summarize defined model\r\n",
        "  # model.summary()\r\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OoCA00AtGx9",
        "outputId": "939d3032-5fb1-4451-f759-4a1382056467"
      },
      "source": [
        "# Main\r\n",
        "\r\n",
        "path = \"/content/fulldataset_IA20.xlsx\"\r\n",
        "dataset = pd.read_excel(path, sheet_name=0, header=0, dtype={'score': np.int32})\r\n",
        "dataset = dataset.fillna(0)\r\n",
        "\r\n",
        "# Tokenizing and Padding the reviews\r\n",
        "tokenizer, sequences, word_index = tokenizing_review(dataset['review'])\r\n",
        "\r\n",
        "# lista das colunas de Produtos/Servicos a serem aprendidos e preditos  -  colunas de 17 a 20\r\n",
        "lista_colunas = list(dataset.keys())[17:20]\r\n",
        "\r\n",
        "# monta o dataset, treina e salva\r\n",
        "for cada_coluna in lista_colunas:\r\n",
        "  # Splitting the dataset into X and y\r\n",
        "  new_dataset_X, new_dataset_y = montar_dataset(dataset['score'], sequences, dataset[cada_coluna])\r\n",
        "  # Splitting into train and test\r\n",
        "  X_train, X_test, y_train, y_test = train_test_split(new_dataset_X, new_dataset_y, test_size=0.20, random_state=42)\r\n",
        "  \r\n",
        "  # Choosing the model\r\n",
        "  model = model1(max_length=global_max_length+1, vocab_size=global_vocab_size)  \r\n",
        "  hist = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test), verbose=0)\r\n",
        "\r\n",
        "  # Showing the Baseline Error\r\n",
        "  print_baseline_error(model, cada_coluna, X_test, y_test)\r\n",
        "\r\n",
        "  # Saving the model\r\n",
        "  model.save(\"/content/models/\" + cada_coluna + '.h5')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline Error Alívio de Numerário : 0.00%\n",
            "Baseline Error Aplicativo : 33.33%\n",
            "Baseline Error App ourocard : 0.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m302KP0XPiSV",
        "outputId": "6b0088c6-66bd-46e3-9ceb-fa0f48cfad4c"
      },
      "source": [
        "!zip -r \"/content/models.zip\" \"/content/models/\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/models/ (stored 0%)\n",
            "  adding: content/models/Aplicativo.h5 (deflated 64%)\n",
            "  adding: content/models/App ourocard.h5 (deflated 64%)\n",
            "  adding: content/models/Alívio de Numerário.h5 (deflated 64%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}